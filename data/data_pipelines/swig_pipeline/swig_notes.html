https://github.com/jhcho99 / gsrtr:
The code is modified and adapted from these amazing repositories:
•	End-to-End Object Detection with Transformers
•	Grounded Situation Recognition (Allen Institute for Artificial Intelligence)

  https://prior.allenai.org/projects/gsr.
                     SWiG benchmark.
                 https://github.com/allenai/swig
1.	gsrtr/datasets/swig.py
https://github.com/jhcho99/gsrtr/blob/master/SWiG/SWiG_jsons/train_classes.csv
train_classes.csv  : for each role : 3 objets  chosen with sequence number
>>>> can be used for SSL as similar to previous recognized object
Example:
"n00017222": {
    "gloss": ["plant", "flora", "plant life"], 
    "def": "(botany) a living organism lacking the power of locomotion"
    }


https://github.com/jhcho99/CoFormer.
https://github.com/kellyiss/SituFormer


https://github.com/jhcho99/gsrtr/blob/master/datasets/swig.py
	https://pytorch.org/docs/stable/multiprocessing.html:

torch.multiprocessing is a wrapper around the native multiprocessing module. It registers custom reducers, that use shared memory to provide shared views on the same data in different processes. Once the tensor/storage is moved to shared_memory. it will be possible to send it to other processes without making any copies.
It’s enough to change import multiprocessing to import torch.multiprocessing to have all the tensors sent through the queues or shared via other mechanisms, moved to shared memory.
Strategy management:
torch.multiprocessing.get_all_sharing_strategies()
                        Returns a set of sharing strategies supported on a current system.

torch.multiprocessing.get_sharing_strategy()
                        Returns the current strategy for sharing CPU tensors.

torch.multiprocessing.set_sharing_strategy(new_strategy)
                        Sets the strategy for sharing CPU tensors.

Parameters
new_strategy (str) – Name of the selected strategy. Should be one of the values returned by get_all_sharing_strategies().

	from torchvision import transforms
L51:         self.color_change = transforms.Compose([transforms.ColorJitter(hue=0.1, saturation=0.1, brightness=0.1), transforms.RandomGrayscale(p=0.3)])



	import datasets.transforms as T
L115>>>L144 in https://github.com/facebookresearch/detr/blob/main/datasets/coco.py
	Csv files :
https://docs.python.org/fr/3.6/library/csv.html
with open(self.class_list, 'r') as file:
       f= csv.reader(file, delimiter=',')
	self.load_classes                  train.classes.csv
                                      L53,L54
                        self.classes, self.idx_to_class = self.load_classes(csv.reader(file, delimiter=','))

	.json files:  , train.json,    dev.json,   test.json     imsitu_space.json
L56,L57
Json to txt
        With open(self.ann_file) as file:
              self.SWiG_json = json.load(file)

	train.json, 
verb_orders= imsitu_space.json [‘verbs’], 
train_classes.csv[classes, idx_to_class].

self._read_annotations:
self.SWiG_json, verb_info, self.classes
load data respectively from    train.json,   verb_info=verb_orders= imsitu_space.json [“verbs"],  train_classes.csv[classes, idx_to_class].
imsitu_space.json:
imsitu_space.json {"verbs"::{verb :{ "framenet": , def : , abstract : ,  "order":  , roles :  }}
                                 "nouns" :{e_i : {  "gloss": , def :    } , e_j,..  }

The same image may have 6 annotations
while total_anns < 6:    # 

	img_file = f"{self.img_folder}/" + image
result[img_file].append({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'class1': class1, 'class2': class2, 'class3': class3})
all_ims.txt

self.image_data = self._read_annotations(self.SWiG_json, verb_info, self.classes)
self.image_names = list(self.image_data.keys())


	L62 to L65
verb_path  file   : verb_indices.txt
           self.load_verb
role_path file :   role_indices.txt

	Try to index all things (images, roles, classes,verbs,etc)

self.image_to_image_idx = {}
Dict : for each image, we associate an idx.
                           L67>>L71

L67>>L71 : for indexing images


	verb_info=verb_orders= imsitu_space.json [“verbs"],  

for verb, value in verb_info.items()}:
Example of item of verb_info
"tattooing": {"framenet": "Create_physical_artwork", "abstract": "AGENT tattooed TARGET with TOOL in PLACE", "def": "to mark the skin with permanent colors and patterns", "order": ["agent", "target", "tool", "place"], "roles": {"tool": {"framenet": "instrument", "def": "The tool used"}, "place": {"framenet": "place", "def": "The location where the tattoo event is happening"}, "target": {"framenet": "representation", "def": "The entity being tattooed"}, "agent": {"framenet": "creator", "def": "The entity doing the tattoo action"}}}
 # verb_role
 self.verb_role = {verb: value['order'] for verb, value in verb_info.items()}



  # for each verb, the indices of roles in the frame.
  self.vidx_ridx = [[self.role_to_idx[role] for role in self.verb_role[verb]] for verb in self.idx_to_verb]


self.verb_role : associate each verb with each roles :

verb in the list of “verbs” =verb_info.items(1)
value in the list of nouns= verb_info.items(2)
	self.verb_role = {verb: value['order'] for verb, value in verb_info.items()}


	    def load_image(self, image_index):
          im = Image.open(self.image_names[image_index])
          im = im.convert('RGB')
          if self.is_training:
             im = np.array(self.color_change(im))
         else:
             im = np.array(im)
        return im.astype(np.float32) / 255.0
	      def load_annotations(self, image_index):
        # get ground truth annotations
        annotation_list = self.image_data[self.image_names[image_index]]
        annotations = np.ones((0, 7)) * -1

        # some images appear to miss annotations (like image with id 257034)
        if len(annotation_list) == 0:
            return annotations

        # parse annotations
        for idx, a in enumerate(annotation_list):
            annotation = np.ones((1, 7)) * -1  # allow for 3 annotations
            annotation[0, 0] = a['x1']
            annotation[0, 1] = a['y1']
            annotation[0, 2] = a['x2']
            annotation[0, 3] = a['y2']
            annotation[0, 4] = self.name_to_label(a['class1'])
            annotation[0, 5] = self.name_to_label(a['class2'])
            annotation[0, 6] = self.name_to_label(a['class3'])
            annotations = np.append(annotations, annotation, axis=0)

        return annotations


	
import skimage.transform
from torchvision import transforms, utils
def __init__(self, img_folder, ann_file, class_list, verb_path, role_path, verb_info, is_training, transform=None):
   self.transform = transform

def __getitem__(self, idx):
        ……
        if self.transform:
            sample = self.transform(sample)
        return sample
	#image and annotation loading, make the samples in  json format 
    def __getitem__(self, idx):
        img = self.load_image(idx)
        annot = self.load_annotations(idx)
        verb = self.image_names[idx].split('/')[2]
        verb = verb.split('_')[0]

        verb_idx = self.verb_to_idx[verb]
        verb_role = self.verb_info[verb]['order']
        verb_role_idx = [self.role_to_idx[role] for role in verb_role]
        sample = {'img': img, 'annot': annot, 'img_name': self.image_names[idx], 'verb_idx': verb_idx, 'verb_role_idx': verb_role_idx}
        if self.transform:
            sample = self.transform(sample)
        return sample
	def name_to_label(self, name):
        return self.classes[name]

    def num_nouns(self):
        return max(self.classes.values()) + 1

    def image_aspect_ratio(self, image_index):
        image = Image.open(self.image_names[image_index])
        return float(image.width) / float(image.height)
	#Data batching
def collater(data):
	Shift of the image in the predefinced (Batch size, max_height,max_width) tensor
shift_0
shift_1
	util.misc
L271:  util.misc.nested_tensor_from_tensor_list                         ~padding :  the list of tensors(padded_imgs) in one tensor
	util.box_ops
'boxes': util.box_ops.swig_box_xyxy_to_cxcywh(annot[:, :4], mw, mh, gt=True),
def box_xyxy_to_cxcywh(x):
    x0, y0, x1, y1 = x.unbind(-1)
    b = [(x0 + x1) / 2, (y0 + y1) / 2,
         (x1 - x0), (y1 - y0)]
    return torch.stack(b, dim=-1)
	L271>>>>L284 
return(util.misc.nested_tensor_from_tensor_list(padded_imgs),
	            [{'verbs': vi,
	              'roles': vri,
	              'boxes': util.box_ops.swig_box_xyxy_to_cxcywh(annot[:, :4], mw, mh, gt=True), 
	              'labels': annot[:, -3:],
	              'width': w,
	              'height': h,
	              'shift_0': s0,
	              'shift_1': s1,
	              'scale': sc,
	              'max_width': mw,
	              'max_height': mh,
	              'img_name': im}
	
	              for vi, vri, annot, w, h, s0, s1, sc, im in zip(verb_indices, verb_role_indices, annot_padded, widths, heights, shift_0, shift_1, scales, img_names)]   )
	
	

	class Resizer(object):
    def __init__(self, is_for_training):
        self.is_for_training = is_for_training


    def __call__(self, sample, min_side=300, max_side=400):
        image, annots, image_name = sample['img'], sample['annot'], sample['img_name']

  return {'img': torch.from_numpy(new_image), 'annot': torch.from_numpy(annots), 'scale': scale, 'img_name': image_name, 'verb_idx': sample['verb_idx'], 'verb_role_idx': sample['verb_role_idx'], 'shift_1': shift_1, 'shift_0': shift_0}
	class Resizer(object):   L287----->L329                                scale,                       scale_factor
#resize the image with the computed scale
image = skimage.transform.resize(image, (int(round(rows_orig * scale)), int(round((cols_orig * scale)))))
rows, cols, cns = image.shape
	new_image = np.zeros((rows, cols, cns)).astype(np.float32)
new_image[:rows, :cols, :] = image.astype(np.float32)
shift_1 = int((400 - cols) * 0.5)
shift_0 = int((400 - rows) * 0.5)

!= -1 :Not equal
*= : times
annots[:, 0][annots[:, 0] != -1] = annots[:, 0][annots[:, 0] != -1] + shift_1
annots[:, 1][annots[:, 1] != -1] = annots[:, 1][annots[:, 1] != -1] + shift_
annots[:, 2][annots[:, 2] != -1] = annots[:, 2][annots[:, 2] != -1] + shift_1
annots[:, 3][annots[:, 3] != -1] = annots[:, 3][annots[:, 3] != -1] + shift_0

annotation :numpy  >> annotation = np.ones((1, 7)) * -1
image : numpy (PIL, )

return {'img': torch.from_numpy(new_image), 'annot': torch.from_numpy(annots), 'scale': scale, 'img_name': image_name, 'verb_idx': sample['verb_idx'], 'verb_role_idx': sample['verb_role_idx'], 'shift_1': shift_1, 'shift_0': shift_0}
	class Augmenter(object):
    def __call__(self, sample, flip_x=0.5):
        image, annots, img_name = sample['img'], sample['annot'], sample['img_name']

    sample = {'img': image, 'annot': annots, 'img_name': img_name, 'verb_idx': sample['verb_idx'], 'verb_role_idx':  
             sample['verb_role_idx']}

   return sample
	class Augmenter(object):

        If np.random.rand() < flip_x:
	            image = image[:, ::-1, :]
	            rows, cols, channels = image.shape
	
	            x1 = annots[:, 0].copy()
	            x2 = annots[:, 2].copy()
	
	            annots[:, 0][annots[:, 0] != -1] = cols - x2[annots[:, 0] != -1]
	            annots[:, 2][annots[:, 2] != -1] = cols - x1[annots[:, 2] != -1]
	

	class Normalizer(object):
    def __init__(self):
        self.mean = np.array([[[0.485, 0.456, 0.406]]])
        self.std = np.array([[[0.229, 0.224, 0.225]]])

    def __call__(self, sample):
        image, annots = sample['img'], sample['annot']

        return {'img': ((image.astype(np.float32) - self.mean) / self.std), 'annot': annots, 'img_name': sample['img_name'],  
                'verb_idx': sample['verb_idx'], 'verb_role_idx': sample['verb_role_idx']}

	class Normalizer(object) (-mean/std)
self.mean = np.array([[[0.485, 0.456, 0.406]]])
self.std = np.array([[[0.229, 0.224, 0.225]]])
'img': ((image.astype(np.float32) - self.mean) / self.std)
	class UnNormalizer(object):
    def __init__(self, mean=None, std=None):
        if mean is None:
            self.mean = [0.485, 0.456, 0.406]
        else:
            self.mean = mean
        if std is None:
            self.std = [0.229, 0.224, 0.225]
        else:
            self.std = std

    def __call__(self, tensor):
        for t, m, s in zip(tensor, self.mean, self.std):
            t.mul_(s).add_(m)
        return tensor
	class UnNormalizer(object):
for t, m, s in zip(tensor, self.mean, self.std):
t.mul_(s).add_(m)                                      torch    tensor
	def build(image_set, args):
    root = Path(args.swig_path)
    img_folder = root / "images_512"

    PATHS = {
        "train": root / "SWiG_jsons" / "train.json",
        "val": root / "SWiG_jsons" / "dev.json",
        "test": root / "SWiG_jsons" / "test.json",
    }
    ann_file = PATHS[image_set]

    classes_file = Path(args.swig_path) / "SWiG_jsons" / "train_classes.csv"
    verb_path = Path(args.swig_path) / "SWiG_jsons" / "verb_indices.txt"
    role_path = Path(args.swig_path) / "SWiG_jsons" / "role_indices.txt"

    with open(f'{args.swig_path}/SWiG_jsons/imsitu_space.json') as f:
        all = json.load(f)
        verb_orders = all['verbs']

    is_training = (image_set == 'train')

    TRANSFORMS = {
        "train": transforms.Compose([Normalizer(), Augmenter(), Resizer(True)]),
        "val": transforms.Compose([Normalizer(), Resizer(False)]),
        "test": transforms.Compose([Normalizer(), Resizer(False)]),
    }
    tfs = TRANSFORMS[image_set]
    
    dataset = CSVDataset(img_folder=str(img_folder),
                         ann_file=ann_file,
                         class_list=classes_file,
                         verb_path=verb_path,
                         role_path=role_path,
                         verb_info=verb_orders,
                         is_training=is_training,
                         transform=tfs)

    if is_training:
        args.SWiG_json_train = dataset.SWiG_json
    else:
        if not args.test:
            args.SWiG_json_dev = dataset.SWiG_json
        else:
            args.SWiG_json_test = dataset.SWiG_json

    args.idx_to_verb = dataset.idx_to_verb
    args.idx_to_role = dataset.idx_to_role
    args.idx_to_class = dataset.idx_to_class
    args.vidx_ridx = dataset.vidx_ridx

    return dataset


