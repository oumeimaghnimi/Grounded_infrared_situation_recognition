

 # -----------------------------------------------------------------------------------------------------
 
 ActivityNet SRL (ASRL) was created  from ActivityNet Captions (AC) and ActivityNet Entities (AE) datasets by following the underling steps:
 # ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding 2015.
 # Dense-Captioning Events in Videos 2017.[ActivityNet Captions(AC) dataset]
 # Grounded video description 2019: [ActivityNet-Entities dataset]
     https://github.com/facebookresearch/grounded-video-description
     https://github.com/facebookresearch/ActivityNet-Entities:
            ActivityNet Entities Dataset and Challenge
     https://paperswithcode.com/dataset/activitynet-entities-1
     ./Téléchargements  ~ pc linux/mon PC 

 # Vognet: Video Object Grounding using Semantic Roles in Language Description 2020
 # https://github.com/TheShadow29/vognet-pytorch/:
 # ActivityNet SRL (ASRL) wasused in the context of Video Object Grounding (VOG): localization of  objects in a video referred in a query sentence description.
 # Semantic role of object relations are elevated via spatial and temporal concatenation of contrastive examples sampled from  ActivityNet-SRL (ASRL).
 # https://github.com/facebookresearch/ActivityNet-Entities#activitynet-entities-object-localization-challenge-2020
        - ActivityNet Entities Object Localization (Grounding) Challenge joins the official ActivityNet Challenge as a guest task in 2021.

     http://activity-net.org/challenges/2020/challenge.html(7 tasks in 2021)
       - Temporal Action Localization (ActivityNet)
       - Dense-Captioning Events in Videos (ActivityNet Captions).
       - Trimmed Activity Recognition (Kinetics)
       - Spatio-temporal Action Localization (AVA)
       - Activity Detection in Extended Videos Sequestered Data Leaderboard (ActEV SDL)
       - HACS Temporal Action Localization Challenge 2020
        - ActivityNet Entities Object Localization task(ActivityNet-Entities)


 
    1. Add semantic roles to captions in AC using SRL Labeling system from AllenAI, which is itself a re-implementation of a deep BiLSTM model (He et al, 2017): 
                   https://github.com/TheShadow29/vognet-pytorch/tree/master/dcode/sem_role_labeller.py
                       http://docs.allennlp.org/v0.9.0/api/allennlp.models.semantic_role_labeler.html
                       https://algorithmia.com/algorithms/allenai/semantic_role_labeling/docs
                       https://demo.allennlp.org/semantic-role-labeling
                       https://allenai.org/allennlp
                       https://github.com/allenai/allennlp        
                       https://github.com/facebookresearch/fairseq       
                   Bert based semantic-role labeling system:Simple bert models for relation extraction and semantic role labeling, 2019.
                   And the implementation provided is  trained on OntoNotes5 which uses the PropBank annotation format
                   Allennlp: A deep semantic natural language processing platform
                       The obtained semantic-roles are cleaned using heuristics like removing verbs without any roles usually for “is”, “are” etc.
                   spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing, 2017.
                   The Stanford CoreNLP natural language processing toolkit 2014
                        https://github.com/stanfordnlp/CoreNLP
                        https://github.com/stanfordnlp/huggingface-models
   

    1. Prepocess AE. In particular, resize all the proposals, ground-truth bounding boxes (this is required for SPAT/TEMP).
    1. Preprocess the features and choose only 5 groundtruths for GT5 setting.
    1. Obtain the bounding boxes and category names from AE for the relevant phrases.
    1. Filter out some verbs like "is", "are", "complete", "begin"
    1. Filter some SRL Arguments based on Frequency.
    1. Get Training/Validation/Test videos.
    1. Do Contrastive Sampling and store the dictionary files for easier sampling during training.
    
#https://github.com/facebookresearch/fairseq                 
# Microsoft COCO Caption Evaluation: https://github.com/jiasenlu/coco-caption
     Evaluation codes for MS COCO caption generation.
      See cocoEvalCapDemo.ipynb

# ----------------------------------------------------------------------------------------------------- 

# Preparing Data

 download the data through   download_data.sh:

Optional: set the data folder.
```
cd $ROOT/data
bash download_data.sh all [data_folder]
```

After everything is downloaded successfully, the folder structure should look like:

```
data
|-- anet (530gb)
    |-- anet_detection_vg_fc6_feat_100rois.h5
    |-- anet_detection_vg_fc6_feat_100rois_resized.h5
    |-- anet_detection_vg_fc6_feat_gt5_rois.h5
    |-- fc6_feat_100rois
    |-- fc6_feat_5rois
    |-- rgb_motion_1d
|-- anet_cap_ent_files (31M)
    |-- anet_captions_all_splits.json
    |-- anet_ent_cls_bbox_trainval.json
    |-- csv_dir
	 |-- train.csv
	 |-- train_postproc.csv
	 |-- val.csv
	 |-- val_postproc.csv
    |-- dic_anet.json
|-- anet_srl_files (112M)
    |-- arg_vocab.pkl
    |-- trn_asrl_annots.csv
    |-- trn_srl_obj_to_index_dict.json
    |-- val_asrl_annots.csv
    |-- val_srl_obj_to_index_dict.json
```

It should ~530 gb of data !!

NOTE: Highly advisable to have the features in SSD; otherwise massive drop in speed!


# Pre-Trained Models
   Google Drive Link for all models: https://drive.google.com/open?id=1e3FiX4FTC8n6UrzY9fTYQzFNKWHihzoQ
   Also, see individual models (with corresponding logs) at EXPTS.md: https://github.com/TheShadow29/vognet-pytorch/blob/master/EXPTS.md

