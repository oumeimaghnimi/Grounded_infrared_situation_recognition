Co-training Transformer with Videos and Images Improves Action Recognition 2021.
https://ai.googleblog.com/2022/03/co-training-transformer-with-videos-and.html:
         Bowen Zhang, Student Researcher and Jiahui Yu, Senior Research Scientist, Google Research, Brain Team.
      *CoVeR adopts a multi-task learning strategy trained on multiple datasets, each with their own classifier.
         TimeSFormer, Video SwinTransformer, TokenLearner, ViViT, MoViNet, VATT, VidTr, and OmniSource 

  https://docs.trymito.io/
    Mito: the fastest way to do Python data analytics, simply by editing a spreadsheet.

0-1 Infar Dataset: infrared action recognition at different times, 2020.

0-2Transferable Feature Representation for Visible-to-Infrared Cross-Dataset Human Action Recognition 2019

1-Actions can be represented using various data modalities:
          RGB, skeleton, depth, infrared, point cloud,event stream, audio, acceleration, radar, and WiFi signal,
          which encode different sources of useful yet distinct information and have various advantages depending on the application scenarios.
          
 Consequently, we will attempt to investigate different types of approaches for action reconition using various modalities.

#https://www.di.ens.fr/~miech/datasetviz/
Preprocessing multi-modal data:

   1.1-Thermal Infrared Object Tracking Benchmark:LSOTB: infrared object  tracking
   
   1.2-LLVIP dataset can contribute to the community of computer vision by promoting image fusion, pedestrian detection and image-to-image translation 
   in very low-light applications （The wavelength: 8~14um (thermal infrared images)):pix2pixGAN, imagefusion_densefuse, FusionGAN.
      https://github.com/bupt-ai-cz/LLVIP
      
                     VEDAI : Vehicle detection in aerial imagery: a small target detection benchmark, 2015.(vehicle detection in aerial imagery)
                     FLIR : https://www.flir.com/oem/adas/adas- dataset- form/ .
                     KAIST: Multispectral pedestrian detec- tion: benchmark dataset and baselines, 2015.

  
   2-NTU RGB+D 60∕120: RGB, skeleton, depth, infrared (action recognition, early prediction) 
       https://github.com/hikvision-research/skelact/tree/main/skelact
   3-PKU MMD  dataset (action detection):
                      https://github.com/hhe-distance/AIF-CNN
    
   4-Human_Action_Recognition HAR dataset:
                    https://www.kaggle.com/datasets/meetnagadia/human-action-recognition-har-dataset
                    https://dphi.tech/challenges/data-sprint-76-human-activity-recognition/233/data
                    
   5-ucf-101
   6-HACS Human Action Clips and Segments Dataset for Recognition and Temporal Localization  
   7-AVA (Spatio-temporal Action Localization dataset)
   8-HMDB51: A Large Video Database for Human Motion Recognition
   9-Moments in Time Dataset: one million videos for event understanding  

      SomethingSomething-V2 (SSv2) 
   
   10-Kinetics 400/600/700(Kinetics Human Action Video Dataset): skeleton with openpose, RGB:
                     https://pytorch.org/vision/stable/generated/torchvision.datasets.Kinetics.html
                     https://github.com/open-mmlab/mmaction2/blob/master/tools/data/kinetics/README.md
                     https://github.com/open-mmlab/mmskeleton/blob/master/doc/SKELETON_DATA.md
                     https://github.com/CMU-Perceptual-Computing-Lab/openpose: 
                       - OpenPose has represented the first real-time multi-person system to jointly detect human body,
                                   hand, facial, and foot keypoints (in total 135 keypoints) on single images.
                    https://docs.openvino.ai/2021.2/omz_demos_python_demos_human_pose_estimation_demo_README.html           
   11-Posetics
   12- VGG Human Pose: The VGG Human Pose Estimation datasets is a set of large video datasets annotated with human upper-body pose
   
    
    
    
    
   13-Charades: This dataset guides our research into unstructured video activity recognition and commonsense reasoning for daily human activities
  
   14-Activity detection in Extended Videos Sequestered Data  ActEV  SDL from Multiview Extended Video with Activities (MEVA) dataset
   15-Oops! dataset: Predicting Unintentional Action in Video
   16-ActivityNet ( Human Activity Understanding) 
   
   17-SWiG benchmark 
   18-ActivityNet Captions:
   19-ActivityNet SRL (ASRL)
   20-MSR-VTT: A Large Video Description Dataset for Bridging Video and Language
   21-VideoMCC : a New Benchmark for Video Comprehension
   
      - Named-entity recognition platforms
          Notable NER platforms include:

               *GATE supports NER across many languages and domains out of the box, usable via a graphical interface and a Java API.
               *OpenNLP includes rule-based and statistical named-entity recognition.
               *SpaCy features fast statistical NER as well as an open-source named-entity visualizer.
  
           *HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips 2019.
                HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video  Clips. 2019.

           *End-to-End Learning of Visual Representations from Uncurated Instructional Videos 2019
           *End to end dense video captionning with masked transformers 2018.
           *ALIGN : Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision 2021.
           *CLIP(Contrastive Language-Image Pre-Training): Learning Transferable Visual Models From Natural Language Supervision 2021.
           *Labelling unlabelled videos from scratch with multi-modal self-supervision 2020.
           *Multi-modal self supervised learning from videos 2021(EPFL)
           *Broaden your views for self supervised video learning 2021(EPFL)
           *Fine-grained Multi-Modal Self-Supervised Learning 2021.
           *VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text 2021.
           *Self-Supervised MultiModal Versatile Networks 2021 :
             https://github.com/deepmind/deepmind-research/tree/master/mmv

          *Explore and Match: End-to-End Video Grounding with Transformer 2022.
          *TransVG: End-to-End Visual Grounding with Transformers 2021
          *Visual Grounding with Transformers 2021
          *Grounded Situation Recognition with Transformers 2021.
          *End-to-End Dense Video Grounding via Parallel Regression 2021.
         *On Pursuit of Designing Multi-modal Transformer for Video Grounding 2021.
         *Multi-View Transformer for 3D Visual Grounding 2022.
         *Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue System 2019
            (audio+visual)
         *UniT: Multimodal Multitask Learning with a Unified Transformer
         *Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation 2021.
          Reference:
              *https://mmf.sh/docs
              * https://github.com/facebookresearch/mmf
              * Vision-language Pre-training: Basics, recent Advances, and Future Trends 2021.
         * Vision-language Pre-training: Basics, recent Advances, and Future Trends 2021.
         * Vision-and-Language Pretrained Models: A Survey, 2022.
  
   21-Hand Gesture Recognition datasets: 
                   https://aiia.csd.auth.gr/auth-uav-gesture-dataset/  ( infrared, skeleton)
                   Multi-Modal Dataset for Hand Gesture Recognition:
                        https://www.kaggle.com/datasets/gti-upm/multimodhandgestrec
                  20BN-JESTER: Human Hand Gestures Dataset
   22-unconstrained face recognition:Labeled Faces in the Wild dataset(LFW Face Database)
      Charlotte-ThermalFace: A Fully Annotated Thermal Infrared Face Dataset with Various Environmental Conditions and Distances 2022.
      unconstrained face recognition:Labeled Faces in the Wild dataset(LFW Face Database)
      
   23-https://github.com/open-mmlab/     openMM3D for 3D, point cloud  linkedin.
   23-KITTI
  
   24-DAVIS Densely Annotated VIdeo Segmentation dataset
  
   25-cityscapes: https://www.cityscapes-dataset.com/
   26- Fire and Smoke Dataset - V7 Labs:https://www.v7labs.com/open-datasets/fire-and-smoke-dataset
   27-The Large-scale Scene Understanding (LSUN) challenge aims to provide a different benchmark for large-scale scene classification and understanding. 
         (can be used for image generation)
   28-The LabelMe-12-50k dataset
   
   29-https://github.com/EgocentricVision
     https://github.com/EgocentricVision#datasets, EPIC kitchen, etc: see for pfe, #repositories.
     ego4d-data
     
   30-The Defense Systems Information Analysis Center (DSIAC) is a component of the U.S. Department of Defense’s (DoD's) Information Analysis Center (IAC):
         https://dsiac.org/tag/infrared-ir-imaging/
         https://dsiac.org/technical-inquiries/notable/infrared-imagery-of-tactical-vehicles-personnel/
         Available Thermal, Hyperspectral, or Polarimetric Imagery Datasets
         Infrared Imagery Datasets
         ATR algorithm
  
*github.com//jinwchoi/awesome-action-recognition*
     Action Recognition Datasets
           -Video Dataset Overview from Antoine Miech
           -HACS
           -Moments in Time, paper
           -AVA, paper, [INRIA web] for missing videos
           -Kinetics, paper, download toolkit
           -OOPS - A dataset of unintentional action, paper
           -COIN - a large-scale dataset for comprehensive instructional video analysis, paper
           -YouTube-8M, technical report
           -YouTube-BB, technical report
           -DALY Daily Action Localization in Youtube videos. Note: Weakly supervised action detection dataset. Annotations consist of start and end time of each action, one bounding box per each action per video.
           -20BN-JESTER, 20BN-SOMETHING-SOMETHING
           -ActivityNet Note: They provide a download script and evaluation code here .
           -Charades
           -Charades-Ego, paper - First person and third person video aligned dataset
           -EPIC-Kitchens, paper - First person videos recorded in kitchens. Note they provide download scripts and a python library here
           -Sports-1M - Large scale action recognition dataset.
           -THUMOS14 Note: It overlaps with UCF-101 dataset.
           -THUMOS15 Note: It overlaps with UCF-101 dataset.
           -HOLLYWOOD2: Spatio-Temporal annotations
           -UCF-101, annotation provided by THUMOS-14, and corrupted annotation list, UCF-101 corrected annotations and different version annotaions. And there are also some pre-computed spatiotemporal action detection results
           -UCF-50.
           -UCF-Sports, note: the train/test split link in the official website is broken. Instead, you can download it from here.
           -HMDB
           -J-HMDB
           -LIRIS-HARL
           -KTH
           -MSR Action Note: It overlaps with KTH datset.
           -Sports Videos in the Wild
           -NTU RGB+D
           -Mixamo Mocap Dataset
           -UWA3D Multiview Activity II Dataset
           -Northwestern-UCLA Dataset
           -SYSU 3D Human-Object Interaction Dataset
           -MEVA (Multiview Extended Video with Activities) Dataset
       Video Annotation
           -Efficiently scaling up crowdsourced video annotation - C. Vondrick et. al, IJCV2013. [code]
           -The Design and Implementation of ViPER - D. Mihalcik and D. Doermann, Technical report.
           -VTT: Visual Object Tagging Tool. Modern app to annotate objects in videos and images. It facilitates the development of an end-to-end machine learning pipeline encompassing the annotation/export/import of assets. Moreover, it could run as a native app or via web.
           -VIA: VGG Image Annotator. Simple and standalone manual annotation web-app for image, audio and video. It runs in the web browser and does not require any installation or setup.
       Object Recognition
           -Object Detection
           -Video Object Detection
                 Detect to Track and Track to Detect] - C. Feichtenhofer et al., ICCV2017. [code], [project web]
                     https://www.robots.ox.ac.uk/~vgg/research/detect-track/
                     https://github.com/feichtenhofer/detect-track
      Video Object Detection Datasets
          -ImageNet VID
          -YouTube-8M, technical report
          -YouTube-BB, technical report

      Pose Estimation
         -AlphaPose - PyTorch based realtime and accurate pose estimation and tracking tool from SJTU.
         -Detect-and-Track: Efficient Pose Estimation in Videos - R. Girdhar et al., arXiv2017.
         -OpenPose Library - Caffe based realtime pose estimation library from CMU.
         -Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields - Z. Cao et al, CVPR2017. [code] depends on the [caffe RT pose] - Earlier version of OpenPose from CMU.
         -DensePose [code] - Dense pose human estimation in the wild implemented in the Detectron framework.
         -MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network - M. Kocabas et al, ECCV2018. [code]
         -DeepLabCut: markerless pose estimation of user-defined body parts with deep learning - A. Mathis et al, Nature Neuroscience 2018. [code]
      Competitions
         -ActEV (Activities in Extended Video - Activity detection in security camera videos. Runs through 2021. Hosted by NIST.


 








  



  
 
  
         
##downloading data        
         
 bash download_data.sh ${DATASET}
 
 '''
 cd $ROOT/data
 bash download_data.sh all [data_folder]
  '''   


'''
git clone https://github.com/TheShadow29/vognet-pytorch.git
cd vognet-pytorch
export ROOT=$(pwd)
cd $ROOT/data
bash download_data.sh all [data_folder]



'''
     

