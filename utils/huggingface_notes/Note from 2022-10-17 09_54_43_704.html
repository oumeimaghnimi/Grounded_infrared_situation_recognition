<html xmlns:tomboy="http://beatniksoftware.com/tomboy" xmlns:size="http://beatniksoftware.com/tomboy/size" xmlns:link="http://beatniksoftware.com/tomboy/link"><head><META http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Note from 2022-10-17 09:54:43.704</title><style type="text/css">
	body {  }
	h1 { font-size: xx-large;
     	     font-weight: bold;
     	     border-bottom: 1px solid black; }
	div.note {
		   position: relative;
		   display: block;
		   padding: 5pt;
		   margin: 5pt; 
		   white-space: -moz-pre-wrap; /* Mozilla */
 	      	   white-space: -pre-wrap;     /* Opera 4 - 6 */
 	      	   white-space: -o-pre-wrap;   /* Opera 7 */
 	      	   white-space: pre-wrap;      /* CSS3 */
 	      	   word-wrap: break-word;      /* IE 5.5+ */ }
	</style></head><body><div class="note" id="Note from 2022-10-17 09:54:43.704"><a name="note from 2022-10-17 09:54:43.704"></a><h1>Note from 2022-10-17 09:54:43.704</h1>pfe: <a style="color:#3465A4" href="https://huggingface.co/blog/train-decision-transformers">https://huggingface.co/blog/train-decision-transformers</a>
simulation 3D
mixed reality
logistics
diffusers
unified transformer
Gato
whatsapp wajdi
SLAM 3D rendering
Interpretable
Tableau
data engineering

MTEB: Massive Text Embedding Benchmark: <a style="color:#3465A4" href="https://huggingface.co/blog/mteb">https://huggingface.co/blog/mteb</a>

diffuseur models to generate data Infrared
Named entity recognition:  select description of the videos from Named entity recognition models in huggingface.
Pretrained models similar to our specific task,  fine tuning with different fine-tuning schemes: see DINO may be.
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model ‚Äî one as close as possible to the task you have at hand ‚Äî and fine-tune it.
see also <a style="color:#3465A4" href="https://github.com/allenai/swig,">https://github.com/allenai/swig,</a> <a style="color:#3465A4" href="https://github.com/TheShadow29/vognet-pytorch,">https://github.com/TheShadow29/vognet-pytorch,</a> <a style="color:#3465A4" href="https://github.com/jhcho99/,">https://github.com/jhcho99/,</a> 
pfe: <a style="color:#3465A4" href="https://">https://</a> meduim.com/standford-cs224w/graphs-are-all-you-need-generating-multi-modal-representations-for-vqa-744a8a1ad448:
multi-modal graph Networks for compositonal generalization in visual questionning answering (Princeton university, NIPS 2020)
idea: predicts the trajectory of the eye from the input image.(AI-Scolar)

Next task:  try to put ViT, Detr into  commun plateform,  pipeline
Surveillance system
extend Classifying each word in a sentence tasks to videos
See it : good try to use your dataset: see blogs can help you/ colab 

<a style="color:#3465A4" href="https://huggingface.co/blog/how-to-train-sentence-transformers,">https://huggingface.co/blog/how-to-train-sentence-transformers,</a>    <a style="color:#3465A4" href="https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/95_Training_Sentence_Transformers.ipynb">https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/95_Training_Sentence_Transformers.ipynb</a>

<a style="color:#3465A4" href="https://huggingface.co/blog/vision-transformers:">https://huggingface.co/blog/vision-transformers:</a> Deep Dive: Vision Transformers On Hugging Face Optimum Graphcore:

show how easy it is to fine-tune pre-trained Transformer models for your dataset using the Hugging Face Optimum library on Graphcore Intelligence Processing Units (IPUs). As an example, we will show a step-by-step guide and provide a notebook that takes a large, widely-used chest X-ray dataset and trains a vision transformer (ViT) model.

see            ZeroShotClassificationPipeline
                 ZeroShotImageClassificationPipeline
                 ZeroShotObjectDetectionPipeline

<a style="color:#3465A4" href="https://git-lfs.github.com/">https://git-lfs.github.com/</a>

prepare other data a faire enchallah
imitate dowload.sh to prepare dataloading file. see colab course also ho&lt; to download, web scraping, etc.Kaggle

<a style="color:#3465A4" href="https://github.com/huggingface/diffusers:">https://github.com/huggingface/diffusers:</a> notebooks and demo
Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch  : for infarred generation

Multiple types of models, such as UNet, can be used as building blocks in an end-to-end diffusion system (see src/diffusers/models).
Training examples to show how to train the most popular diffusion model tasks (see examples, e.g. unconditional-image-generation).


Get pro Huggingface/ google colab

fastai X Hugging Face Group 2022

How to convert a  Transformers model to TensorFlow?

Deploying ViT on Vertex AI: see Google/AWS courses, data engineering, <a style="color:#3465A4" href="https://huggingface.co/blog/deploy-vertex-ai">https://huggingface.co/blog/deploy-vertex-ai</a>
                                                     Deploying ViT on Kubernetes with TF Serving: <a style="color:#3465A4" href="https://huggingface.co/blog/deploy-tfserving-kubernetes">https://huggingface.co/blog/deploy-tfserving-kubernetes</a>

 
Try to write every remarque in an article for a thesis.


See blogs in huggingface: <a style="color:#3465A4" href="https://huggingface.co/blog">https://huggingface.co/blog</a>
<a style="color:#3465A4" href="https://huggingface.co/blog/setfit:">https://huggingface.co/blog/setfit:</a> SetFit: Efficient Few-Shot Learning Without Prompts: Few-shot learning with pretrained language models has emerged as a promising solution to every data scientist's nightmare: dealing with data that has few to no labels
<a style="color:#3465A4" href="https://huggingface.co/blog/train-decision-transformers.">https://huggingface.co/blog/train-decision-transformers.</a>



Note: any function in huggingface face see all its parameters in the hub

19-10-2020-Huggingfaceplateform

üìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.
üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.
üó£Ô∏è Audio: automatic speech recognition and audio classification.
üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.






The task specific pipelines: see all available pipelines in <a style="color:#3465A4" href="https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.pipeline,">https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.pipeline,</a> <a style="color:#3465A4" href="https://huggingface.co/docs/transformers/main_classes/pipelines">https://huggingface.co/docs/transformers/main_classes/pipelines</a>

CONCEPTUAL GUIDES:

http<a style="color:#3465A4" href="s://huggingface.co/docs/transformers/model_summary:">s://huggingface.co/docs/transformers/model_summary:</a>
Each one of the models in the library falls into one of the following categories:

autoregressive-models
autoencoding-models
seq-to-seq-models
multimodal-models
retrieval-based-models



http<a style="color:#3465A4" href="s://huggingface.co/docs/transformers/tokenizer_summary">s://huggingface.co/docs/transformers/tokenizer_summary</a>
http<a style="color:#3465A4" href="s://github.com/huggingface/tokenizers">s://github.com/huggingface/tokenizers</a>
Subword tokenization
Byte-Pair Encoding (BPE)
Byte-level BPE
WordPiece
Unigram
SentencePiece
http<a style="color:#3465A4" href="s://huggingface.co/docs/transformers/bertology">s://huggingface.co/docs/transformers/bertology</a>
http<a style="color:#3465A4" href="s://huggingface.co/docs/transformers/perplexity">s://huggingface.co/docs/transformers/perplexity</a>

API describes all classes and functions:

MAIN CLASSES details the most important classes like configuration, model, tokenizer, and pipeline.
MODELS details the classes and functions related to each model implemented in the library.
INTERNAL HELPERS details utility classes and functions used internally.



for vid√©o on huggingface:(Try to apply them)

VideoMAE (from Multimedia Computing Group, Nanjing University) released with the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.
X-CLIP (from Microsoft Research) released with the paper Expanding Language-Image Pretrained Models for General Video Recognition by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.

Try to use all models

Add a neurology module into model


see Tutorials

see how to build and share demos
Train training in SageMaker
etc

http<a style="color:#3465A4" href="s://github.com/huggingface/accelerate">s://github.com/huggingface/accelerate</a>
http<a style="color:#3465A4" href="s://github.com/huggingface/datasets">s://github.com/huggingface/datasets</a>



courses on Transformers:
I-  to use, Google Colab notebook for windows users especially : Colab courses:  <a style="color:#3465A4" href="https://colab.research.google.com/notebooks/intro.ipynb">https://colab.research.google.com/notebooks/intro.ipynb</a>
   
run system commands by preceding them with the ! character.

see  Autres ressources section:


                Utiliser les notebooks dans Colab

                Importer des biblioth√®ques et installer des d√©pendances
               Enregistrer et charger des notebooks dans GitHub
                Formulaires interactifs
               Widgets interactifsNew

                Utiliser les donn√©es
              Chargement de donn√©es : Drive, Sheets et Google Cloud Storage
              Graphiques : visualiser les donn√©es
              Premiers pas avec BigQuery
               Cours d'initiation au Machine Learning
 des notebooks de la formation Google en ligne sur le machine learning. Consultez la formation compl√®te en ligne pour en savoir plus.

             Pr√©sentation du DataFrame pandas
           R√©gression lin√©aire avec tf.keras et des donn√©es synth√©tiques

            Utiliser le mat√©riel acc√©l√©r√©
             TensorFlow avec des GPU
            TensorFlow avec des TPU

II-working_with_files_package_os.ipynb: <a style="color:#3465A4" href="https://github.com/oumeimaghnimi/Huggingface_hub_learning/blob/master/working_with_files_package_os.ipynb">https://github.com/oumeimaghnimi/Huggingface_hub_learning/blob/master/working_with_files_package_os.ipynb</a>
III-
<a style="color:#3465A4" href="https://huggingface.co/course/chapter0/setup:">https://huggingface.co/course/chapter0/setup:</a>  set up your environment so we can try the code yourself.

We‚Äôll cover two ways of setting up your working environment, using a Colab notebook or a Python virtual environment.
we will not be covering the Windows system. If you‚Äôre running on Windows, we recommend following along using a Colab notebook. If you‚Äôre using a Linux distribution or macOS, you can use either approach described here.

create an account: <a style="color:#3465A4" href="https://huggingface.co/welcome">https://huggingface.co/welcome</a>

                                     Accelerate
                                     Amazon SageMaker
                                      course
                                     datasets
                                     datasets-server
                                    diffusers
                                   Evaluate
                                   Gradio
                                    Hub
                                   Hub client library
                                   Inference API
                                 Inference Endpoints
                                 Optimum
                                  Simulate
                                 Tasks
                                  Tokenizers
                                Transformers
                                   Timm


Hub client library:  <a style="color:#3465A4" href="https://huggingface.co/docs/huggingface_hub/index,">https://huggingface.co/docs/huggingface_hub/index,</a>  <a style="color:#3465A4" href="https://github.com/huggingface/huggingface_hub">https://github.com/huggingface/huggingface_hub</a>

                        Delete and clone a repository, and create and update a branch.
                         Download an entire repository and use regex matching to filter and download specific files.
                       Upload your files with a context manager or use a helper to push files to a remote repository.
                       Search thousands of models and datasets on the Hub with specific filters and parameters to only return the best results.
                      Access the Inference API for accelerated inference.
                      Interact with Discussions and Pull Requests .




Getting started with our git and git-lfs interface

  1- If you need to create a repo from the command line (skip if you created a repo from the website).

   $pip install huggingface_hub
    #You already have it if you installed transformers or datasets
    #Install the huggingface_hub library with pip in your environment:
   #$python -m pip install huggingface_hub
# with conda
#conda install -c conda-forge huggingface_hub   #<a style="color:#3465A4" href="https://huggingface.co/docs/huggingface_hub/quick-start">https://huggingface.co/docs/huggingface_hub/quick-start</a>


Once you have successfully installed the huggingface_hub library, log in to your Hugging Face account:

   $huggingface-cli login
  #Log in using a token from huggingface.co/settings/tokens:   http<a style="color:#3465A4" href="s://huggingface.co/settings/tokens">s://huggingface.co/settings/tokens</a> (your Settings page.),     http<a style="color:#3465A4" href="s://huggingface.co/docs/hub/security-tokens">s://huggingface.co/docs/hub/security-tokens</a> _User access tokens.
                       #Organizations API Tokens
                       # We can show/hide my hugginglearners's token:


See http<a style="color:#3465A4" href="s://huggingface.co/docs/hub/models-adding-libraries,">s://huggingface.co/docs/hub/models-adding-libraries,</a> http<a style="color:#3465A4" href="s://huggingface.co/docs/huggingface_hub/quick-start">s://huggingface.co/docs/huggingface_hub/quick-start</a>

Alternatively, if you prefer working from a Jupyter or Colaboratory notebook, login with notebook_login:



&gt;&gt;&gt;from huggingface_hub import notebook_login
&gt;&gt;&gt;notebook_login()

notebook_login will launch a widget in your notebook from which you can enter your Hugging Face credentials.


  #Create a model or dataset repo from the CLI if needed
  $huggingface-cli repo create repo_name --type {model, dataset, space}

#create_repo
#The create_repo method creates a repository on the Hub. Use the name parameter to provide a name for your repository:
#&gt;&gt;&gt;from huggingface_hub import create_repo
#&gt;&gt;&gt;create_repo(repo_id="test-model")
   # output will be: 'http<a style="color:#3465A4" href="s://huggingface.co/lysandre/test-model'">s://huggingface.co/lysandre/test-model'</a>
#When you check your Hugging Face account, you should now see a test-model repository under your namespace.

2-Clone your model or dataset locally
    #Make sure you have git-lfs installed
    #(http<a style="color:#3465A4" href="s://git-lfs.github.com">s://git-lfs.github.com</a>)
$git lfs install
$git clone http<a style="color:#3465A4" href="s://huggingface.co/username/repo_name">s://huggingface.co/username/repo_name</a>

3- Then add, commit and push any file you want, including larges files.
#save files via `.save_pretrained()` or move them here
$git add .
$git commit -m "commit from $USER"
$git push

#upload_file #         see other utiliies of using the hub in <a style="color:#3465A4" href="https://huggingface.co/docs/huggingface_hub/v0.10.1/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file">https://huggingface.co/docs/huggingface_hub/v0.10.1/en/package_reference/hf_api#huggingface_hub.HfApi.upload_file</a>
                                                                                                     <a style="color:#3465A4" href="https://huggingface.co/docs:">https://huggingface.co/docs:</a>     Hub Client library --&gt;&gt;&gt;reference _huggingface Face Hub API
( path_or_fileobj: typing.Union[str, bytes, typing.BinaryIO]path_in_repo: strrepo_id: strtoken: typing.Optional[str] = Nonerepo_type: typing.Optional[str] = Nonerevision: typing.Optional[str] = Noneidentical_ok: typing.Optional[bool] = Nonecommit_message: typing.Optional[str] = Nonecommit_description: typing.Optional[str] = Nonecreate_pr: typing.Optional[bool] = Noneparent_commit: typing.Optional[str] = None ) ‚Üí str

#The upload_file method uploads files to the Hub. This method requires the following:

          # - A path to the file to upload.
           #- The final path in the repository.
           #- The repository you wish to push the files to.

  # &gt;&gt;&gt;from huggingface_hub import upload_file
  #&gt;&gt;&gt; upload_file(
  #&gt;&gt;&gt;path_or_fileobj="/home/lysandre/dummy-test/README.md", 
  #&gt;&gt;&gt;path_in_repo="README.md", 
  #&gt;&gt;&gt;repo_id="lysandre/test-model"
#)
#'http<a style="color:#3465A4" href="s://huggingface.co/lysandre/test-model/blob/main/README.md'">s://huggingface.co/lysandre/test-model/blob/main/README.md'</a>

Good:
Note: If you need to upload more than one file, look at the utilities offered by the Repository class : http<a style="color:#3465A4" href="s://huggingface.co/docs/huggingface_hub/package_reference/repository">s://huggingface.co/docs/huggingface_hub/package_reference/repository</a>
   -  Managing local and online repositories

Once again, if you check your Hugging Face account, you should see the file inside your repository.







http<a style="color:#3465A4" href="s://huggingface.co/docs/hub/models-libraries:">s://huggingface.co/docs/hub/models-libraries:</a>

In most cases, if you're using one of the compatible libraries, your repo will then be accessible from code, through its identifier: username/repo_name

For example for a transformers model, anyone can load it with:

tokenizer = AutoTokenizer.from_pretrained("username/repo_name")
model = AutoModel.from_pretrained("username/repo_name")



Integrated libraries:

AllenNLP: http<a style="color:#3465A4" href="s://github.com/allenai/allennlp.">s://github.com/allenai/allennlp.</a>
Asteroid: http<a style="color:#3465A4" href="s://github.com/asteroid-team/asteroid">s://github.com/asteroid-team/asteroid</a>
docTR: Optical Character Recognition made seamless &amp; accessible to anyone, powered by TensorFlow 2 &amp; PyTorch: http<a style="color:#3465A4" href="s://github.com/mindee/doctr.">s://github.com/mindee/doctr.</a>
ESPnet: end-to-end speech processing toolkit: http<a style="color:#3465A4" href="s://github.com/espnet/espnet.">s://github.com/espnet/espnet.</a>
fastai: http<a style="color:#3465A4" href="s://github.com/fastai/fastai:">s://github.com/fastai/fastai:</a> Library to train fast and accurate models with state-of-the-art outputs.
Keras: http<a style="color:#3465A4" href="s://huggingface.co/docs/hub/keras">s://huggingface.co/docs/hub/keras</a>
Flair: http<a style="color:#3465A4" href="s://github.com/flairNLP/flair">s://github.com/flairNLP/flair</a>
ML-Agents: http<a style="color:#3465A4" href="s://github.com/huggingface/ml-agents:">s://github.com/huggingface/ml-agents:</a>  This is a Fork of the Unity ML-Agents toolkit., which nables games and simulations made with Unity to serve as environments for training intelligent agents.
                                                                                 This version allows you to publish your trained agents in one line of code to the Hugging Face Hub, download powerful agents from the community,
                                                                                 and watch a replay of your agent without using the Unity Editor.
  http<a style="color:#3465A4" href="s://github.com/NVIDIA/NeMo:">s://github.com/NVIDIA/NeMo:</a> Conversational AI toolkit built for researchers (automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS)). 
                                                                The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new conversational AI models.
Pyannote: http<a style="color:#3465A4" href="s://github.com/pyannote/pyannote-audio:">s://github.com/pyannote/pyannote-audio:</a> Neural building blocks for speaker diarization.
PyCTCDecode:http<a style="color:#3465A4" href="s://github.com/kensho-technologies/pyctcdecode">s://github.com/kensho-technologies/pyctcdecode</a> Language model supported CTC decoding for speech recognition.
Pythae: http<a style="color:#3465A4" href="s://github.com/clementchadebec/benchmark_VAE.">s://github.com/clementchadebec/benchmark_VAE.</a> Unifyed framework for Generative Autoencoders in Python.
RL-Baselines3-Zoo: http<a style="color:#3465A4" href="s://github.com/DLR-RM/rl-baselines3-zoo:">s://github.com/DLR-RM/rl-baselines3-zoo:</a> Training framework for Reinforcement Learning, using Stable Baselines3.                                                                          
Sentence Transformers: http<a style="color:#3465A4" href="s://github.com/UKPLab/sentence-transformers:">s://github.com/UKPLab/sentence-transformers:</a>  Multilingual Sentence, Paragraph, and Image Embeddings using BERT &amp; Co.
spaCy: http<a style="color:#3465A4" href="s://github.com/explosion/spaCy:">s://github.com/explosion/spaCy:</a> Advanced Natural Language Processing in Python and Cython.
Scikit Learn (using skops): Machine Learning in Python: http<a style="color:#3465A4" href="s://skops.readthedocs.io/en/stable/">s://skops.readthedocs.io/en/stable/</a>
Speechbrain:  A PyTorch Powered Speech Toolkit.http<a style="color:#3465A4" href="s://speechbrain.github.io/">s://speechbrain.github.io/</a>
Stable-Baselines3: Set of reliable implementations of deep reinforcement learning algorithms in PyTorch. http<a style="color:#3465A4" href="s://github.com/DLR-RM/stable-baselines3.">s://github.com/DLR-RM/stable-baselines3.</a>
TensorFlowTTS: http<a style="color:#3465A4" href="s://github.com/TensorSpeech/TensorFlowTTS:">s://github.com/TensorSpeech/TensorFlowTTS:</a> Real-Time State-of-the-art Speech Synthesis for Tensorflow 2.
Timm: http<a style="color:#3465A4" href="s://github.com/rwightman/pytorch-image-models">s://github.com/rwightman/pytorch-image-models</a> Collection of image models, scripts, pretrained weights, etc.

If you‚Äôre interested in adding your library:
      Integrate your library with the Hub: http<a style="color:#3465A4" href="s://huggingface.co/docs/hub/models-adding-libraries">s://huggingface.co/docs/hub/models-adding-libraries</a>


Integrating the Hub with your library provides many benefits, including:

Free model hosting for you and your users.
Built-in file versioning - even for huge files - made possible by Git-LFS: http<a style="color:#3465A4" href="s://git-lfs.github.com/">s://git-lfs.github.com/</a>
All public models are powered by the Inference API.
    Integrate into your apps over 50,000 pre-trained state of the art models, or your own private models, via simple HTTP requests, with 2x to 10x faster inference than out of the box deployment, and scalability built-in:
                      http<a style="color:#3465A4" href="s://github.com/huggingface/api-inference-community/">s://github.com/huggingface/api-inference-community/</a>
                     http<a style="color:#3465A4" href="s://huggingface.co/docs/api-inference/index">s://huggingface.co/docs/api-inference/index</a>

                     http<a style="color:#3465A4" href="s://api-inference.huggingface.co/dashboard/:">s://api-inference.huggingface.co/dashboard/:</a> Check out the API Usage Dashboard (beta) where you can monitor your usage and requests, per model.

In-browser widgets allow users to interact with your hosted models directly.



Download files from the Hub:     see also http<a style="color:#3465A4" href="s://huggingface.co/docs/huggingface_hub/quick-start">s://huggingface.co/docs/huggingface_hub/quick-start</a>
                                                                           http<a style="color:#3465A4" href="s://huggingface.co/docs/huggingface_hub/v0.10.1/en/package_reference/file_download#huggingface_hub.hf_hub_download">s://huggingface.co/docs/huggingface_hub/v0.10.1/en/package_reference/file_download#huggingface_hub.hf_hub_download</a>   #  see parameters of it 

                                                                                      huggingface_hub.hf_hub_download:   http<a style="color:#3465A4" href="s://github.com/huggingface/huggingface_hub/blob/v0.10.1/src/huggingface_hub/file_download.py#L846">s://github.com/huggingface/huggingface_hub/blob/v0.10.1/src/huggingface_hub/file_download.py#L846</a>
                                                                                                      Parameters: 
                                                                                                                         ( repo_id: strfilename: strsubfolder: typing.Optional[str] = Nonerepo_type: typing.Optional[str] = Nonerevision: typing.Optional[str] = Nonelibrary_name: typing.Optional[str] = Nonelibrary_version: typing.Optional[str] =   
                                                                                                                           Nonecache_dir: typing.Union[str, pathlib.Path, NoneType] = Noneuser_agent: typing.Union[typing.Dict, str, NoneType] = Noneforce_download: typing.Optional[bool] = Falseforce_filename: typing.Optional[str] = 
                                                                                                                            Noneproxies: typing.Optional[typing.Dict] = Noneetag_timeout: typing.Optional[float] = 10resume_download: typing.Optional[bool] = Falseuse_auth_token: typing.Union[bool, str, NoneType] = Nonelocal_files_only: 
                                                                                                                           typing.Optional[bool] = Falselegacy_cache_layout: typing.Optional[bool] = False )

Integration allows users to download your hosted files directly from the Hub using your library.
Use the hf_hub_download function to retrieve a URL and download files from your repository. Downloaded files are stored in your cache: ~/.c<a style="color:#3465A4" href="ache/huggingface/hub.">ache/huggingface/hub.</a> 
You don‚Äôt have to re-download the file the next time you use it, and for larger files, this can save a lot of time. Furthermore, if the repository is updated with a new version of the file, huggingface_hub will automatically download the latest version and store it in the cache for you.
 Users don‚Äôt have to worry about updating their files.

For example, download the config.json file from the lysandre/arxiv-nlp repository:
&gt;&gt;&gt;from huggingface_hub import hf_hub_download
&gt;&gt;&gt;hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json")

Download a specific version of the file by specifying the revision parameter. The revision parameter can be a branch name, tag, or commit hash.
The commit hash must be a full-length hash instead of the shorter 7-character commit hash:
&gt;&gt;&gt;from huggingface_hub import hf_hub_download
&gt;&gt;&gt;hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="877b84a8f93f2d619faa2a6e514a32beef88ab0a")

Good for using it:
Use the cache_dir parameter to change where a file is stored:
&gt;&gt;&gt; from huggingface_hub import hf_hub_download
&gt;&gt;&gt;hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", cache_dir="/home/lysandre/test")

Code sample: (good)

We recommend adding a code snippet to explain how to use a model in your downstream library: http<a style="color:#3465A4" href="s://github.com/huggingface/hub-docs/blob/main/js/src/lib/interfaces/Libraries.ts">s://github.com/huggingface/hub-docs/blob/main/js/src/lib/interfaces/Libraries.ts</a>

Add a code snippet by updating the Libraries Typescript file with instructions for your model. For example, the Asteroid integration includes a brief code snippet for how to load and use an Asteroid model:
 Example1:
 const asteroid = (model: ModelData) =&gt;
`from asteroid.models import BaseModel
model = BaseModel.from_pretrained("${model.id}")`;
Examples: http<a style="color:#3465A4" href="s://github.com/huggingface/hub-docs/blob/main/js/src/lib/interfaces/Libraries.ts">s://github.com/huggingface/hub-docs/blob/main/js/src/lib/interfaces/Libraries.ts</a>

const fairseq = (model: ModelData) =&gt;
	`from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub
models, cfg, task = load_model_ensemble_and_task_from_hf_hub(
    "${model.id}"
)`;


Doing so will also add a tag to your model so users can quickly identify models from your library.



Lastly, it is important to add a model card so users understand how to use your model. See the following link for more details about how to create a model card:
        http<a style="color:#3465A4" href="s://huggingface.co/docs/hub/model-cards">s://huggingface.co/docs/hub/model-cards</a>


Set up the Inference API


huggingface Inference API powers models uploaded to the Hub through your library.

All third-party libraries are Dockerized, so you can install the dependencies you‚Äôll need for your library to work correctly. Add your library to the existing Docker images by navigating to the Docker images folder.
http<a style="color:#3465A4" href="s://github.com/huggingface/api-inference-community">s://github.com/huggingface/api-inference-community</a>
http<a style="color:#3465A4" href="s://github.com/huggingface/api-inference-community/tree/main/docker_images">s://github.com/huggingface/api-inference-community/tree/main/docker_images</a>
for each library:      
     Dockerfile
      requirements.txt
      prestart.sh
     app
     tests
Example: http<a style="color:#3465A4" href="s://github.com/huggingface/api-inference-community/tree/main/docker_images/allennlp">s://github.com/huggingface/api-inference-community/tree/main/docker_images/allennlp</a>



1- Copy the common folder and rename it with the name of your library (e.g. docker/common to docker/your-awesome-library).

2- There are four files you need to edit:

List the packages required for your library to work in requirements.txt.

Update app/main.py with the tasks supported by your model (see here for a complete list of available tasks). Look out for the IMPLEMENT_THIS flag to add your supported task.


ALLOWED_TASKS: Dict[str, Type[Pipeline]] = {
    "token-classification": TokenClassificationPipeline
}


For each task your library supports, modify the app/pipelines/task_name.py files accordingly. We have also added an IMPLEMENT_THIS flag in the pipeline files to guide you. If there isn‚Äôt a pipeline that supports your task, feel free to add one. Open an issue here, and we will be happy to help you.

Add your model and task to the tests/test_api.py file. For example, if you have a text generation model:


TESTABLE_MODELS: Dict[str,str] = {
    "text-generation": "my-gpt2-model"
}

3-Finally, run the following test to ensure everything works as expected:


pytest -sv --rootdir docker_images/your-awesome-library/docker_images/your-awesome-library/

With these simple but powerful methods, you brought the full functionality of the Hub into your library. Users can download files stored on the Hub from your library with hf_hub_download, create repositories with create_repo, and upload files with upload_file. You also set up Inference API with your library, allowing users to interact with your models on the Hub from inside a browser.


Hugging Face Hub documentation:  http<a style="color:#3465A4" href="s://huggingface.co/docs/hub/index">s://huggingface.co/docs/hub/index</a>
                                                                 http<a style="color:#3465A4" href="s://huggingface.co/docs/huggingface_hub/index:">s://huggingface.co/docs/huggingface_hub/index:</a> Hub client library: The huggingface_hub library allows you to interact with the Hugging Face Hub, , a machine learning platform for creators and collaborators.
                                                                 Discover pre-trained models and datasets for your projects or play with the hundreds of machine learning apps hosted on the Hub. You can also create and share your own models and datasets with the community. The huggingface_hub library 
                                                                     provides a simple way to do all these things with Python.


Repositories:
   Introduction
   Getting Started
   Repository Settings
   Pull requests and Discussions
  Next Steps
   Licenses

Models
              Introduction
             The Model Hub
              Model Cards
             Libraries
             Tasks
             Uploading Models
             downloading  Models
            Widgets
            Inference API

Datasets
           Introduction
          Datasets Overview
          Dataset Cards
          Gated Datasets
         Dataset viewer
         Using Datasets
         Adding New Datasets.


Spaces
        Introduction
       Spaces Overview
       Gradio Spaces
      Streamlit Spaces
      Static HTML Spaces
     Custom Python Spaces
     Reference
     Changelog
     Advanced Topics.

Other
      Organizations
      Digital Object Identifier (DOI)
      Security
     Hub API Endpoints
     Contributor Code of Conduct
    Content Guidelines



 
Note: 
contribution guide : help answer questions on issues, and request new features you think will improve the librarary
                    How to contribute to huggingface_hub, the GitHub repository?:  <a style="color:#3465A4" href="https://github.com/huggingface/huggingface_hub/blob/main/CONTRIBUTING.md">https://github.com/huggingface/huggingface_hub/blob/main/CONTRIBUTING.md</a>
                   Contributors should also be respectful of the huggingface code of conduct  :         <a style="color:#3465A4" href="https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md">https://github.com/huggingface/huggingface_hub/blob/main/CODE_OF_CONDUCT.md</a>












Using a Python virtual environment:

           -install Python on your system: <a style="color:#3465A4" href="https://realpython.com/installing-python/">https://realpython.com/installing-python/</a>
           -ensure that it is correctly installed before proceeding to the next steps:

               python --version

              #you should think of the program running your command as the ‚Äúmain‚Äù Python on your system. We recommend keeping this main installation free of any packages, and using it to create separate environments for each application you work on ‚Äî this way, each application can 
             #have its own dependencies and packages, and you won‚Äôt need to worry about potential compatibility issues with other applications.
            # virtual environments, which are self-contained directory trees that each contain a Python installation with a particular Python version alongside all the packages the application needs.
            # Creating such a virtual environment can be done with a number of different tools, but we‚Äôll 
            #use the official Python package for that purpose, which is called venv.
            #<a style="color:#3465A4" href="https://docs.python.org/3/tutorial/venv.html">https://docs.python.org/3/tutorial/venv.html</a>
            #<a style="color:#3465A4" href="https://docs.python.org/3/library/venv.html#module-venv">https://docs.python.org/3/library/venv.html#module-venv</a>
            # can check also: <a style="color:#3465A4" href="https://pip.pypa.io/warnings/venv">https://pip.pypa.io/warnings/venv</a>
         
            #First, create the directory you‚Äôd like your application to live in ‚Äî for example, you might want to make a new directory called transformers-course at the root of your home directory:
            
            mkdir <a style="color:#3465A4" href="~/transformers-course">~/transformers-course</a>
             cd <a style="color:#3465A4" href="~/transformers-course">~/transformers-course</a>

         #From inside this directory, create a virtual environment using the Python venv module:
          
            python -m venv .env
        #You should now have a directory called .env in your otherwise empty folder:
           
           ls -a

         #You can jump in and out of your virtual environment with the activate and deactivate scripts:

         # Activate the virtual environment
          source .env/bin/activate

       # Deactivate the virtual environment
          source .env/bin/deactivate

       #You can make sure that the environment is activated by running the which python command: if it points to the virtual environment, then you have successfully activated it!
          which python

      #Installing dependencies
       
       pip install "transformers[sentencepiece]"

      #You‚Äôre now all set up and ready to go!





Using a Google Colab notebook:

            !pip install transformers
            import transformers
           #This installs a very light version of Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we‚Äôll be using a lot of different features of the library, we recommend installing the development version, which comes       
           with all #the required dependencies for pretty much any imaginable use case:
          !pip install transformers[sentencepiece]
          #This will take a bit of time, but then you‚Äôll be ready to go for the rest of the course!
           #Note: 
            #A simply jupyter notebook for verifying that the colab-env module works as intended in the real world!:    <a style="color:#3465A4" href="https://github.com/oumeimaghnimi/Huggingface_hub_learning/blob/master/colab_env_testbed.ipynb:">https://github.com/oumeimaghnimi/Huggingface_hub_learning/blob/master/colab_env_testbed.ipynb:</a> 




<a style="color:#3465A4" href="https://huggingface.co/course/chapter2/transformers">https://huggingface.co/course/chapter2/transformers</a> models

This course: Requires a good knowledge of Python

Is better taken after an introductory deep learning course, such as fast.ai‚Äôs Practical Deep Learning for Coders (<a style="color:#3465A4" href="https://course.fast.ai/">https://course.fast.ai/</a>)or one of the programs developed by DeepLearning.AI(<a style="color:#3465A4" href="https://www.deeplearning.ai/">https://www.deeplearning.ai/</a>)
Does not expect prior PyTorch or TensorFlow knowledge, though some familiarity with either of those will help.
checking out DeepLearning.AI‚Äôs Natural Language Processing Specialization, which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about.
Natural Language Processing with Transformers, Revised Editionby Lewis Tunstall, Leandro von Werra, Thomas Wolf Released June 2022

 Gradio, an open-source Python library that has been used to build  machine learning demos. Gradio was acquired by Hugging Face, which is where Abubakar now serves as a machine learning team lead.


1-1Natural Language Processing:

The following is a list of common NLP tasks, with some examples of each:

Classifying whole sentences: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not.

Classifying each word in a sentence: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization).

Generating text content: Completing a prompt with auto-generated text, filling in the blanks in a text with masked words;

Extracting an answer from a text: Given a question and a context, extracting the answer to the question based on the information provided in the context;

Generating a new sentence from an input text: Translating a text into another language, summarizing a text.

NLP isn‚Äôt limited to written text though. It also tackles complex challenges in speech recognition and computer vision, such as generating a transcript of an audio sample or a description of an image.

Note: see models related to  classifying each word in a sentence for our  situation surveillance.

1-2-Transformers, what can they do?
      In google Clab notebook:   <a style="color:#3465A4" href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb">https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb</a>
      SageMaker studiolab notebook:     <a style="color:#3465A4" href="https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb">https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter1/section3.ipynb</a>

     The first tool  from the Transformers library: pipeline() function.

     Transformer library: <a style="color:#3465A4" href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>   provides the functionality to create and use those shared models.

     <a style="color:#3465A4" href="https://huggingface.co/models:">https://huggingface.co/models:</a> contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!

     The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want.


       The most basic object in the  Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:
   
     

There are three main steps involved when you pass some inputs to a pipeline:

                  The input is preprocessed into a format the model can understand.
                  The preprocessed inputs are passed to the model.
                  The predictions of the model are post-processed, so you can make sense of them.

Some of the currently available pipelines are:
                                              <a style="color:#3465A4" href="https://huggingface.co/docs/transformers/main_classes/pipelines">https://huggingface.co/docs/transformers/main_classes/pipelines</a>
                    AudioClassificationPipeline
                    AutomaticSpeechRecognitionPipeline
                    ConversationalPipeline
                    DocumentQuestionAnsweringPipeline

                    FeatureExtractionPipeline

                   FillMaskPipeline

                   ImageClassificationPipeline
                   ImageSegmentationPipeline
                  ImageToTextPipeline
                  ObjectDetectionPipeline
                  QuestionAnsweringPipeline
                  SummarizationPipeline
                  TableQuestionAnsweringPipeline
                  TextClassificationPipeline
                 TextGenerationPipeline
                 Text2TextGenerationPipeline
                 TokenClassificationPipeline
                 TranslationPipeline
                  VisualQuestionAnsweringPipeline 

                 ZeroShotClassificationPipeline
                 ZeroShotImageClassificationPipeline
                 ZeroShotObjectDetectionPipeline

                sentiment-analysis
                ner (named entity recognition)


Zero shot learning:

 This pipeline is called zero-shot because you don‚Äôt need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!
A more challenging task where we need to classify texts that haven‚Äôt been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don‚Äôt have to rely on the labels of the pretrained model. You‚Äôve already seen how the model can classify a sentence as positive or negative using those two labels ‚Äî but it can also classify the text using any other set of labels you like.

#This pipeline is called zero-shot because you don‚Äôt need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!

from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)


By default, the pipeline selects a particular pretrained model that has been fine-tuned for the specific task . 

The model is downloaded and cached when you create the  object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.

Example:

from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")


Using any model from the Hub in a pipeline:
         The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task 
          Go to the Model Hub  <a style="color:#3465A4" href="https://huggingface.co/models">https://huggingface.co/models</a> and click on the corresponding tag on the left to display only the supported models for that task.  "  <a style="color:#3465A4" href="https://huggingface.co/models?pipeline_tag=">https://huggingface.co/models?pipeline_tag=</a> text-generation  "    .

Example:
       
           from transformers import pipeline

           generator = pipeline("text-generation", model="distilgpt2")
          generator(
                   "In this course, we will teach you how to",
                    max_length=30,
         num_return_sequences=2,
)

Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. Let‚Äôs look at an example:


from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
Copied
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC).

We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped ‚ÄúHugging‚Äù and ‚ÄúFace‚Äù as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the preprocessing even splits some words into smaller parts. For instance, Sylvain is split into four pieces: S, ##yl, ##va, and ##in. In the post-processing step, the pipeline successfully regrouped those pieces.
          






<a style="color:#3465A4" href="https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py">https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py</a>
class PipelineDataFormat:
    """
    Base class for all the pipeline supported data format both for reading and writing. Supported data formats
    currently includes:
    - JSON
    - CSV
    - stdin/stdout (pipe)
    `PipelineDataFormat` also includes some utilities to work with multi-columns like mapping from datasets columns to
    pipelines keyword arguments through the `dataset_kwarg_1=dataset_column_1` format.
    Args:
        output_path (`str`, *optional*): Where to save the outgoing data.
        input_path (`str`, *optional*): Where to look for the input data.
        column (`str`, *optional*): The column to read.
        overwrite (`bool`, *optional*, defaults to `False`):
            Whether or not to overwrite the `output_path`.
    """

class CsvPipelineDataFormat(PipelineDataFormat):
    """
    Support for pipelines using CSV data format.
    Args:
        output_path (`str`, *optional*): Where to save the outgoing data.
        input_path (`str`, *optional*): Where to look for the input data.
        column (`str`, *optional*): The column to read.
        overwrite (`bool`, *optional*, defaults to `False`):
            Whether or not to overwrite the `output_path`.
    """

class JsonPipelineDataFormat(PipelineDataFormat):
    """
    Support for pipelines using JSON file format.
    Args:
        output_path (`str`, *optional*): Where to save the outgoing data.
        input_path (`str`, *optional*): Where to look for the input data.
        column (`str`, *optional*): The column to read.
        overwrite (`bool`, *optional*, defaults to `False`):
            Whether or not to overwrite the `output_path`.
    """

class PipedPipelineDataFormat(PipelineDataFormat):
    """
    Read data from piped input to the python process. For multi columns data, columns should separated by \t
    If columns are provided, then the output will be a dictionary with {column_x: value_x}
    Args:
        output_path (`str`, *optional*): Where to save the outgoing data.
        input_path (`str`, *optional*): Where to look for the input data.
        column (`str`, *optional*): The column to read.
        overwrite (`bool`, *optional*, defaults to `False`):
            Whether or not to overwrite the `output_path`.
    """
Try to keep the inputs/outputs very simple and ideally JSON-serializable as it makes the pipeline usage very easy without requiring users to understand new kind of objects. 
It‚Äôs also relatively common to support many different types of arguments for ease of use (audio files, can be filenames, URLs or pure bytes)


"""
class _ScikitCompat(ABC):
    """
    Interface layer for the Scikit and Keras compatibility.
    """

    @abstractmethod
    def transform(self, X):
        raise NotImplementedError()

    @abstractmethod
    def predict(self, X):
        raise NotImplementedError()




 Huggingface/API/main classes:

<a style="color:#3465A4" href="https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.pipeline">https://huggingface.co/docs/transformers/v4.23.1/en/main_classes/pipelines#transformers.pipeline</a>


Pipeline chunk batching:

"""

class ChunkPipeline(Pipeline):  #<a style="color:#3465A4" href="https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py">https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py</a>




zero-shot-classification and question-answering are slightly specific in the sense, that a single input might yield multiple forward pass of a model. Under normal circumstances, this would yield issues with batch_size argument.

In order to circumvent this issue, both of these pipelines are a bit specific, they are ChunkPipeline instead of regular Pipeline. In short:


preprocessed = pipe.preprocess(inputs)
model_outputs = pipe.forward(preprocessed)
outputs = pipe.postprocess(model_outputs)


Now becomes:

all_model_outputs = []
for preprocessed in pipe.preprocess(inputs):
    model_outputs = pipe.forward(preprocessed)
    all_model_outputs.append(model_outputs)
outputs = pipe.postprocess(all_model_outputs)

This should be very transparent to your code because the pipelines are used in the same way.

This is a simplified view, since the pipeline can handle automatically the batch to ! Meaning you don‚Äôt have to care about how many forward passes you inputs are actually going to trigger, you can optimize the batch_size independently of the inputs. The caveats from the previous section still apply.



Pipeline custom code:

If you want to override a specific pipeline.

Don‚Äôt hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most cases, so transformers could maybe support your use case.

If you want to try simply you can:

Subclass your pipeline of choice

class MyPipeline(TextClassificationPipeline):
    def postprocess():
        # Your code goes here
        scores = scores * 100
        # And here


my_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)
# or if you use *pipeline* function, then:
my_pipeline = pipeline(model="xxxx", pipeline_class=MyPipeline)

That should enable you to do all the custom code you want.



CONTRIBUTE
     -How to contribute to transformers?
     -How to add a model to Transformers?
    -How to convert a Transformers model to TensorFlow?
   -How to add a pipeline to  Transformers?
  -Testing
  -Checks on a Pull Request

                                                                                         Implementing a pipeline:

huggingface/CONTRIBUTE:


I- How to create a custom pipeline?

In this guide, we will see how to create a custom pipeline and share it on the Hub or add it to the Transformers library.

First and foremost, you need to decide the raw entries the pipeline will be able to take. It can be strings, raw bytes, dictionaries or whatever seems to be the most likely desired input. 

Try to keep these inputs as pure Python as possible as it makes compatibility easier (even through other languages via JSON). Those will be the inputs of the pipeline (preprocess).

Then define the outputs. Same policy as the inputs. The simpler, the better. Those will be the outputs of postprocess method.

Start by inheriting the base class Pipeline. with the 4 methods needed to implement preprocess, _forward, postprocess and _sanitize_parameters.

class MyPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "maybe_arg" in kwargs:
            preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, inputs, maybe_arg=2):
        model_input = Tensor(inputs["input_ids"])
        return {"model_input": model_input}

    def _forward(self, model_inputs):
        # model_inputs == {"model_input": model_input}
        outputs = self.model(**model_inputs)
        # Maybe {"logits": Tensor(...)}
        return outputs

    def postprocess(self, model_outputs):
        best_class = model_outputs["logits"].softmax(-1)
        return best_class





<a style="color:#3465A4" href="https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py">https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py</a>

from ..utils  import  add_end_docstrings

PIPELINE_INIT_ARGS = r"""
    Arguments:
        model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):
            The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from
            [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.
        tokenizer
        ......
        ......
        .....
"""

@add_end_docstrings(PIPELINE_INIT_ARGS)
class Pipeline(_ScikitCompat):

    def __init__(
        self,
       put the the arguments in PIPELINE_INIT_ARGS):(model, tokenizer, etc)


   def save_pretrained(self, save_directory: str):
  def transform(self, X):
  def predict(self, X):
  @contextmanager
  def device_placement(self):
 def ensure_tensor_on_device(self, **inputs):
 def _ensure_tensor_on_device(self, inputs, device):
 def check_model_type(self, supported_models: Union[List[str], dict]):

 @abstractmethod
 def _sanitize_parameters(self, **pipeline_parameters):
  @abstractmethod
  def preprocess(self, input_: Any, **preprocess_parameters: Dict) -&gt; Dict[str, GenericTensor]:
  @abstractmethod
  def _forward(self, input_tensors: Dict[str, GenericTensor], **forward_parameters: Dict) -&gt; ModelOutput:
  @abstractmethod
   def postprocess(self, model_outputs: ModelOutput, **postprocess_parameters: Dict) -&gt; Any:

def get_inference_context(self):

def forward(self, model_inputs, **forward_params):

def get_iterator(
        self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params
    ):

def __call__(self, inputs, *args, num_workers=None, batch_size=None, **kwargs):

def run_multi(self, inputs, preprocess_params, forward_params, postprocess_params):
def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
 def iterate(self, inputs, preprocess_params, forward_params, postprocess_params):


Compare:

Note:  pay attention on  the difference between  def _forward and def forward


_sanitize_parameters exists to allow users to pass any parameters whenever they wish, be it at initialization time pipeline(...., maybe_arg=4) or at call time pipe = pipeline(...); output = pipe(...., maybe_arg=4).

1-
   @abstractmethod
    def _sanitize_parameters(self, **pipeline_parameters):
        """
        _sanitize_parameters will be called with any excessive named arguments from either `__init__` or `__call__`
        methods. 
 The returns of _sanitize_parameters are the 3 dicts of kwargs that will be passed directly to preprocess, _forward and postprocess. 
Don‚Äôt fill anything if the caller didn‚Äôt call with any extra parameter. That allows to keep the default arguments in the function definition which is always     more ‚Äúnatural‚Äù.
        It is not meant to be called directly, it will be automatically called and the final parameters resolved by
        `__init__` and `__call__`
        """
        raise NotImplementedError("_sanitize_parameters not implemented")

"""
Example for  class ImageClassificationPipeline(Pipeline):

 def _sanitize_parameters(self, top_k=None):
        postprocess_params = {}
        if top_k is not None:
            postprocess_params["top_k"] = top_k
        return {}, {}, postprocess_params

"""

A classic example would be a top_k argument in the post processing in classification tasks.


pipe = pipeline("my-new-task")
pipe("This is a test")
[{"label": "1-star", "score": 0.8}, {"label": "2-star", "score": 0.1}, {"label": "3-star", "score": 0.05}
{"label": "4-star", "score": 0.025}, {"label": "5-star", "score": 0.025}]

pipe("This is a test", top_k=2)
[{"label": "1-star", "score": 0.8}, {"label": "2-star", "score": 0.1}]


In order to achieve that, we‚Äôll update our postprocess method with a default parameter to 5. and edit _sanitize_parameters to allow this new parameter.

def postprocess(self, model_outputs, top_k=5):
    best_class = model_outputs["logits"].softmax(-1)
    # Add logic to handle top_k
    return best_class


def _sanitize_parameters(self, **kwargs):
    preprocess_kwargs = {}
    if "maybe_arg" in kwargs:
        preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]

    postprocess_kwargs = {}
    if "top_k" in kwargs:
        postprocess_kwargs["top_k"] = kwargs["top_k"]
    return preprocess_kwargs, {}, postprocess_kwargs

Try to keep the inputs/outputs very simple and ideally JSON-serializable as it makes the pipeline usage very easy without requiring users to understand new kind of objects. It‚Äôs also relatively common to support many different types of arguments for ease of use (audio files, can be filenames, URLs or pure bytes)

2-
    def preprocess(self, input_: Any, **preprocess_parameters: Dict) -&gt; Dict[str, GenericTensor]:
        """
        Preprocess will take the `input_` of a specific pipeline and return a dictionnary of everything necessary for
        `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.
        """
        raise NotImplementedError("preprocess not implemented")

"""
Example for  class ImageClassificationPipeline(Pipeline):
    def preprocess(self, image):
        image = load_image(image)
        model_inputs = self.feature_extractor(images=image, return_tensors=self.framework)
        return model_inputs

"""

3-
      @abstractmethod
    def _forward(self, input_tensors: Dict[str, GenericTensor], **forward_parameters: Dict) -&gt; ModelOutput:
        """
        _forward will receive the prepared dictionnary from `preprocess` and run it on the model. This method might
        involve the GPU or the CPU and should be agnostic to it. Isolating this function is the reason for `preprocess`
        and `postprocess` to exist, so that the hot path, this method generally can run as fast as possible.
        It is not meant to be called directly, `forward` is preferred. It is basically the same but contains additional
        code surrounding `_forward` making sure tensors and models are on the same device, disabling the training part
        of the code (leading to faster inference).
        """
        raise NotImplementedError("_forward not implemented")

"""
   def _forward(self, model_inputs):
        model_outputs = self.model(**model_inputs)
        return model_outputs

"""

4-

    @abstractmethod
    def postprocess(self, model_outputs: ModelOutput, **postprocess_parameters: Dict) -&gt; Any:
        """
        Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into
        something more friendly. Generally it will output a list or a dict or results (containing just strings and
        numbers).
        """
        raise NotImplementedError("postprocess not implemented")

"""
def postprocess(self, model_outputs, top_k=5):
        if top_k &gt; self.model.config.num_labels:
            top_k = self.model.config.num_labels

        if self.framework == "pt":
            probs = model_outputs.logits.softmax(-1)[0]
            scores, ids = probs.topk(top_k)
        elif self.framework == "tf":
            probs = stable_softmax(model_outputs.logits, axis=-1)[0]
            topk = tf.math.top_k(probs, k=top_k)
            scores, ids = topk.values.numpy(), topk.indices.numpy()
        else:
            raise ValueError(f"Unsupported framework: {self.framework}")

        scores = scores.tolist()
        ids = ids.tolist()
        return [{"score": score, "label": self.model.config.id2label[_id]} for score, _id in zip(scores, ids)]

"""




 [huggingface.co/models](<a style="color:#3465A4" href="https://huggingface.co/models?filter=image-classification">https://huggingface.co/models?filter=image-classification</a>).
<a style="color:#3465A4" href="https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/image_classification.py">https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/image_classification.py</a>

from .base import  Pipeline

@add_end_docstrings(PIPELINE_INIT_ARGS)
class ImageClassificationPipeline(Pipeline):
def __init__(self, *args, **kwargs):

 def _sanitize_parameters(self, top_k=None):  

def __call__(self, images: Union[str, List[str], "Image.Image", List["Image.Image"]], **kwargs):

def preprocess(self, image):

def _forward(self, model_inputs):

def postprocess(self, model_outputs, top_k=5):




II-Adding it to the list of supported tasks:

<a style="color:#3465A4" href="https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/__init__.py">https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/__init__.py</a>

                   from .base import   PipelineRegistry

                  PIPELINE_REGISTRY = PipelineRegistry(supported_tasks=SUPPORTED_TASKS, task_aliases=TASK_ALIASES)



def PipelineRegistry: #<a style="color:#3465A4" href="https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py">https://github.com/huggingface/transformers/blob/bbd150e92f84db72e7507d0c3ce69474b2948839/src/transformers/pipelines/base.py</a>

def register_pipeline(
        self,
        task: str,
        pipeline_class: type,
        pt_model: Optional[Union[type, Tuple[type]]] = None,  #example:AutoModelForSequenceClassification,
        tf_model: Optional[Union[type, Tuple[type]]] = None,      
        default: Optional[Dict] = None,
        type: Optional[str] = None,
    ) -&gt; None:
        if task in self.supported_tasks:
            logger.warning(f"{task} is already registered. Overwriting pipeline for task {task}...")

        if pt_model is None:
            pt_model = ()
        elif not isinstance(pt_model, tuple):
            pt_model = (pt_model,)

        if tf_model is None:
            tf_model = ()
        elif not isinstance(tf_model, tuple):
            tf_model = (tf_model,)

        task_impl = {"impl": pipeline_class, "pt": pt_model, "tf": tf_model}

        if default is not None:
            if "model" not in default and ("pt" in default or "tf" in default):
                default = {"model": default}
            task_impl["default"] = default

        if type is not None:
            task_impl["type"] = type

        self.supported_tasks[task] = task_impl
        pipeline_class._registered_impl = {task: task_impl}

"""
To register your new-task to the list of supported tasks, you have to add it to the PIPELINE_REGISTRY:

from transformers.pipelines import PIPELINE_REGISTRY

PIPELINE_REGISTRY.register_pipeline(
    "new-task",
    pipeline_class=MyPipeline,
    pt_model=AutoModelForSequenceClassification,
)


You can specify a default model if you want, in which case it should come with a specific revision (which can be the name of a branch or a commit hash, here we took "abcdef") as well was the type:

PIPELINE_REGISTRY.register_pipeline(
    "new-task",
    pipeline_class=MyPipeline,
    pt_model=AutoModelForSequenceClassification,
    default={"pt": ("user/awesome_model", "abcdef")},
    type="text",  # current support type: text, audio, image, multimodal
)


III-Share your pipeline on the Hub:

To share your custom pipeline on the Hub, you just have to save the custom code of your Pipeline subclass in a python file. For instance, let‚Äôs say we want to use a custom pipeline for sentence pair classification like this:

import numpy as np

from transformers import Pipeline


def softmax(outputs):
    maxes = np.max(outputs, axis=-1, keepdims=True)
    shifted_exp = np.exp(outputs - maxes)
    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)


class PairClassificationPipeline(Pipeline):
    def _sanitize_parameters(self, **kwargs):
        preprocess_kwargs = {}
        if "second_text" in kwargs:
            preprocess_kwargs["second_text"] = kwargs["second_text"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, text, second_text=None):
        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)

    def _forward(self, model_inputs):
        return self.model(**model_inputs)

    def postprocess(self, model_outputs):
        logits = model_outputs.logits[0].numpy()
        probabilities = softmax(logits)

        best_class = np.argmax(probabilities)
        label = self.model.config.id2label[best_class]
        score = probabilities[best_class].item()
        logits = logits.tolist()
        return {"label": label, "score": score, "logits": logits}


The implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If we have saved this in a file named pair_classification.py, we can then import it and register it like this:

from pair_classification import PairClassificationPipeline
from transformers.pipelines import PIPELINE_REGISTRY
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification

PIPELINE_REGISTRY.register_pipeline(
    "pair-classification",
    pipeline_class=PairClassificationPipeline,
    pt_model=AutoModelForSequenceClassification,
    tf_model=TFAutoModelForSequenceClassification,
)


Once this is done, we can use it with a pretrained model. For instance sgugger/finetuned-bert-mrpc has been fine-tuned on the MRPC dataset, which classifies pairs of sentences as paraphrases or not.

from transformers import pipeline

classifier = pipeline("pair-classification", model="sgugger/finetuned-bert-mrpc")

Then we can share it on the Hub by using the save_pretrained method in a Repository:

from huggingface_hub import Repository

repo = Repository("test-dynamic-pipeline", clone_from="{your_username}/test-dynamic-pipeline")
classifier.save_pretrained("test-dynamic-pipeline")
repo.push_to_hub()

This will copy the file where you defined PairClassificationPipeline inside the folder "test-dynamic-pipeline", along with saving the model and tokenizer of the pipeline, before pushing everything in the repository {your_username}/test-dynamic-pipeline. After that anyone can use it as long as they provide the option trust_remote_code=True:



IV-Add the pipeline to Transformers:

If you want to contribute your pipeline to Transformers, you will need to add a new module in the pipelines submodule with the code of your pipeline, then add it in the list of tasks defined in pipelines/__init__.py.

Then you will need to add tests. Create a new file tests/test_pipelines_MY_PIPELINE.py with example with the other tests.

The run_pipeline_test function will be very generic and run on small random models on every possible architecture as defined by model_mapping and tf_model_mapping.

This is very important to test future compatibility, meaning if someone adds a new model for XXXForQuestionAnswering then the pipeline test will attempt to run on it. Because the models are random it‚Äôs impossible to check for actual values, that‚Äôs why There is a helper ANY that will simply attempt to match the output of the pipeline TYPE.

You also need to implement 2 (ideally 4) tests.

test_small_model_pt : Define 1 small model for this pipeline (doesn‚Äôt matter if the results don‚Äôt make sense) and test the pipeline outputs. The results should be the same as test_small_model_tf.
test_small_model_tf : Define 1 small model for this pipeline (doesn‚Äôt matter if the results don‚Äôt make sense) and test the pipeline outputs. The results should be the same as test_small_model_pt.
test_large_model_pt (optional): Tests the pipeline on a real pipeline where the results are supposed to make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make sure there is no drift in future releases
test_large_model_tf (optional): Tests the pipeline on a real pipeline where the results are supposed to make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make sure there is no drift in future releases





The Inference API: 

All the models can be tested directly through your browser using the Inference API, which is available on the Hugging Face website.(<a style="color:#3465A4" href="https://huggingface.co/">https://huggingface.co/</a>)
 You can play with the model directly on this page by inputting custom text and watching the model process the input data.

The Inference API that powers the widget is also available as a paid product, which comes in handy if you need it for your workflows. See the pricing page for more details: <a style="color:#3465A4" href="https://huggingface.co/pricing">https://huggingface.co/pricing</a>

1-3: How do Transformers work? <a style="color:#3465A4" href="https://huggingface.co/course/chapter1/4?fw=pt">https://huggingface.co/course/chapter1/4?fw=pt</a>

A bit of Transformer history:.....

Transformer models grouped into three categories:

GPT-like (also called auto-regressive Transformer models)
BERT-like (also called auto-encoding Transformer models)
BART/T5-like (also called sequence-to-sequence Transformer models)



Transformers are language models: 
All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as language models. This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!

This type of model develops a statistical understanding of the language it has been trained on, but it‚Äôs not very useful for specific practical tasks. 

---&gt;&gt;&gt;Because of this, the general pretrained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way ‚Äî that is, using human-annotated labels ‚Äî on a given task.


                  Task example: predicting the next word in a sentence having read the n previous words.
                                           This is called    Causal language modeling because the outputs depend on the past and present inputs, but not the future ones.

                  masked language modeling: the model predicts a masked word in the sentence.


Transformers are big models:

   Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models‚Äô sizes as well as the amount of data they are pretrained on.
   Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, 
   And this is showing a project for a (very big) model led by a team consciously trying to reduce the environmental impact of pretraining.
  hyperparameter tunning:
 The footprint of running lots of trials to get the best hyperparameters would be even higher.
   Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!
   This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community.
Pretraining:
Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.

his pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.
Fine-tunning:
Fine-tuning, on the other hand, is the training done after a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. 

Wait ‚Äî why not simply train directly for the final task? There are a couple of reasons:

The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).
Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.
For the same reason, the amount of time and resources needed to get good results are much lower.
For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is ‚Äútransferred,‚Äù hence the term transfer learning.
different fine-tuning schemes:
Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model ‚Äî one as close as possible to the task you have at hand ‚Äî and fine-tune it.

General architecture:

The model is primarily composed of two blocks:

Encoder (left): The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
Decoder (right): The decoder uses the encoder‚Äôs representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.

Each of these parts can be used independently, depending on the task:

           Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.
           Decoder-only models: Good for generative tasks such as text generation.
           Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.


Attention layers:
       A key feature of Transformer models is that they are built with special layers called attention layers. 
      All you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.
      The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied.

The original architecture:/training

The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.

To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3.

Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.

The attention mask can also be used in the encoder/decoder to prevent the model from paying attention to some special words ‚Äî for instance, the special padding word used to make all the inputs the same length when batching together sentences.

Architectures vs. checkpoints:

  As we dive into Transformer models in this course, you‚Äôll see mentions of architectures and checkpoints as well as models.

These terms all have slightly different meanings:

Architecture: This is the skeleton of the model ‚Äî the definition of each layer and each operation that happens within the model.
Checkpoints: These are the weights that will be loaded in a given architecture.
Model: This is an umbrella term that isn‚Äôt as precise as ‚Äúarchitecture‚Äù or ‚Äúcheckpoint‚Äù: it can mean both. This course will specify architecture or checkpoint when it matters to reduce ambiguity.
For example, BERT is an architecture while bert-base-cased, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say ‚Äúthe BERT model‚Äù and ‚Äúthe bert-base-cased model.‚Äù









</div></body></html>