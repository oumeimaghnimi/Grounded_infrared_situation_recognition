sometimes  train script is in format .sh 
     bash train.sh
     
     
*Image pretraining examples:
     https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining: 
         Supervised: ViT, Swin Transformer,
         self supervised: SimMIM (by Microsoft Research), MAE (by Facebook AI).

The Trainer class provides an API for feature-complete training in PyTorch for most standard use cases. It‚Äôs used in most of the example scripts.

Before instantiating your Trainer, create a TrainingArguments to access all the points of customization during training:
       a-https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments
         https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/training_args.py#L121

The API supports distributed training on multiple GPUs/TPUs, mixed precision through NVIDIA Apex and Native AMP for PyTorch.

The Trainer contains the basic training loop which supports the above features. To inject custom behavior you can subclass them and override the following methods:

          b-get_train_dataloader ‚Äî Creates the training DataLoader.
          c-get_eval_dataloader ‚Äî Creates the evaluation DataLoader.
          d-get_test_dataloader ‚Äî Creates the test DataLoader.
          e-log ‚Äî Logs information on the various objects watching training.
          f-create_optimizer_and_scheduler ‚Äî Sets up the optimizer and learning rate scheduler if they were not passed at init. Note, that you can also subclass or override the create_optimizer and create_scheduler methods separately.
          g-create_optimizer ‚Äî Sets up the optimizer if it wasn‚Äôt passed at init.
          i-create_scheduler ‚Äî Sets up the learning rate scheduler if it wasn‚Äôt passed at init.
          j-compute_loss - Computes the loss on a batch of training inputs.
          k-training_step ‚Äî Performs a training step.
          l-prediction_step ‚Äî Performs an evaluation/test step.
          m-evaluate ‚Äî Runs an evaluation loop and returns metrics.
          n-predict ‚Äî Returns predictions (with metrics if labels are available) on a test set.
The Trainer class is optimized for ü§ó Transformers models and can have surprising behaviors when you use it on other models. When using it on your own model, make sure:

your model always return tuples or subclasses of ModelOutput.
your model can compute the loss if a labels argument is provided and that loss is returned as the first element of the tuple (if your model returns tuples)
your model can accept multiple label arguments (use the label_names in your TrainingArguments to indicate their name to the Trainer) but none of them should be named "label".
Here is an example of how to customize Trainer to use a weighted loss (useful when you have an unbalanced training set):

Copied
from torch import nn
from transformers import Trainer


class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
Another way to customize the training loop behavior for the PyTorch Trainer is to use callbacks that can inspect the training loop state (for progress reporting, logging on TensorBoard or other ML platforms‚Ä¶) and take decisions (like early stopping).
https://github.com/NVIDIA/apex
         
         
Input -> Tokenization -> Model Inference -> Post-Processing (task dependent) -> Output
  Data_pipeline:                                                     preprocessed = pipe.preprocess(inputs)
  Model_pipeline:                                                    model_outputs = pipe.forward(preprocessed)
  Evaluation, Visualization, Analysis and performance measure        outputs = pipe.postprocess(model_outputs)

 
from transformers import Pipeline

class MyPipeline(Pipeline):

    def _sanitize_parameters(self, **kwargs):
        '''
           def __init__(
                       model: 
                       tokenizer:
                       .....
                      ):
        '''
        preprocess_kwargs = {}
        if "maybe_arg" in kwargs:
            preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, inputs, maybe_arg=2):
        #batching, transformation, indexing, dict_to_json,xml_to_json,txt_to_json,tokenization, input embedding
        model_input = Tensor(inputs["input_ids"])
        
        return {"model_input": model_input}

    def _forward(self, model_inputs):
        # model_inputs == {"model_input": model_input}
        outputs = self.model(**model_inputs)
        # Maybe {"logits": Tensor(...)}
        
       '''May be
        
        class ViTModel(ViTPreTrainedModel):

        def forward(
        self,
        pixel_values: Optional[torch.Tensor] = None,
        bool_masked_pos: Optional[torch.BoolTensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        ):  

 
       return BaseModelOutputWithPooling(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
         )
        '''
    
        return outputs

    def postprocess(self, model_outputs):
        
        best_class = model_outputs["logits"].softmax(-1)
        return best_class
        
        '''getting the representation features from trained model
        
        attentions=encoder_outputs.attentions,
        get attentions of last layer
        attentions = outputs.attentions[-1] 
        nh = attentions.shape[1] # number of heads
  
        '''Functions for visualizing features and attention maps
        def get_attention_maps(pixel_values, attentions, nh):
        def visualize_attention(image):
        '''
'''


#For Named entity recognition pipeline

from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")

--->
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC).

We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped ‚ÄúHugging‚Äù and ‚ÄúFace‚Äù as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the preprocessing even splits some words into smaller parts. For instance, Sylvain is split into four pieces: S, ##yl, ##va, and ##in. In the post-processing step, the pipeline successfully regrouped those pieces.
          
