sometimes  train script is in format .sh 
     bash train.sh


Input -> Tokenization -> Model Inference -> Post-Processing (task dependent) -> Output
  Data_pipeline:                                                     preprocessed = pipe.preprocess(inputs)
  Model_pipeline:                                                    model_outputs = pipe.forward(preprocessed)
  Evaluation, Visualization, Analysis and performance measure        outputs = pipe.postprocess(model_outputs)

 
from transformers import Pipeline

class MyPipeline(Pipeline):

    def _sanitize_parameters(self, **kwargs):
        '''
           def __init__(
                       model: 
                       tokenizer:
                       .....
                      ):
        '''
        preprocess_kwargs = {}
        if "maybe_arg" in kwargs:
            preprocess_kwargs["maybe_arg"] = kwargs["maybe_arg"]
        return preprocess_kwargs, {}, {}

    def preprocess(self, inputs, maybe_arg=2):
        #batching, transformation, indexing, dict_to_json,xml_to_json,txt_to_json,tokenization, input embedding
        model_input = Tensor(inputs["input_ids"])
        
        return {"model_input": model_input}

    def _forward(self, model_inputs):
        # model_inputs == {"model_input": model_input}
        outputs = self.model(**model_inputs)
        # Maybe {"logits": Tensor(...)}
        
       '''May be
        
        class ViTModel(ViTPreTrainedModel):

        def forward(
        self,
        pixel_values: Optional[torch.Tensor] = None,
        bool_masked_pos: Optional[torch.BoolTensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        ):  

 
       return BaseModelOutputWithPooling(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
         )
        '''
    
        return outputs

    def postprocess(self, model_outputs):
        
        best_class = model_outputs["logits"].softmax(-1)
        return best_class
        
        '''getting the representation features from trained model
        
        attentions=encoder_outputs.attentions,
        get attentions of last layer
        attentions = outputs.attentions[-1] 
        nh = attentions.shape[1] # number of heads
  
        '''Functions for visualizing features and attention maps
        def get_attention_maps(pixel_values, attentions, nh):
        def visualize_attention(image):
        '''
'''


#For Named entity recognition pipeline

from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")

--->
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC).

We pass the option grouped_entities=True in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped “Hugging” and “Face” as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the preprocessing even splits some words into smaller parts. For instance, Sylvain is split into four pieces: S, ##yl, ##va, and ##in. In the post-processing step, the pipeline successfully regrouped those pieces.
          
